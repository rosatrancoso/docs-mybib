\documentclass[12pt,a4paper]{article}

\usepackage[nottoc]{tocbibind}

% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{color,soul} % hl

\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{placeins} % FloatBarrier

\usepackage[a4paper]{geometry} % textwidth 418pt (14cm) instead of 345pt (12...cm)
\usepackage{parskip} % Sets page size and margins

\usepackage[colorlinks=true,urlcolor=green,citecolor=red,linkcolor=blue,bookmarks=true]{hyperref}

% package used by \citep and \citet
\usepackage{natbib}
% \usepackage{biblatex} % citetitle

\usepackage[toc,section=section]{glossaries}
\makeglossaries
\input{acronyms}


%Title and author
\title{My bibliography}
\author{Rosa}

%------------- DEFINITIONS -------------%
\def\deg{$^\circ$\xspace}
\def\ms{m s\textsuperscript{-1}\xspace}

%------------- BEGIN -------------%
\begin{document}
\maketitle
\tableofcontents

%------------- START TEXT -------------%
\section{Equations}

\subsection{Correlation Coefficient}

From \url{http://www.cawcr.gov.au/projects/verification/}:

\subsubsection{Pearson Product-Moment Correlation Coefficient}

\begin{equation}
r_{fo} = \frac{\sum {(F-\overline{F})(O-\overline{O})}}{\sqrt{\sum{(F-\overline{F})^2}} \sqrt{\sum{(O - \overline{O})^2}}} = \frac{s^2_{fo}}{s_f s_o}
\end{equation}

Addresses the question: How well did the forecast values correspond to the observed values?

Range: -1 to 1.  Perfect score: 1.

Characteristics: Good measure of linear association or phase error. Visually, the correlation measures how close the points of a scatter plot are to a straight line. Does not take forecast bias into account -- it is possible for a forecast with large errors to still have a good correlation coefficient with the observations. Sensitive to outliers.

\subsubsection{Anomaly Correlation Coefficient}
From \url{https://www.jma.go.jp/jma/jma-eng/jma-center/nwp/outline2013-nwp/pdf/outline2013_Appendix_A.pdf}:

"Anomaly Correlation Coefficient (ACC) is one of the most widely used measures in the verification of spatial fields (Jolliffe and Stephenson 2003), and is the correlation between anomalies of forecasts and those of verifying values with the reference values, such as climatological values. ACC is defined as follows:"

From \url{http://www.cawcr.gov.au/projects/verification/}:

AC Uncentered:

\begin{equation}
AC = \frac{\sum {(F-C)(O-C)}}{\sqrt{\sum{(F-C)^2}} \sqrt{\sum{(O-C)^2}}}    
\end{equation}

Centered:

\begin{equation}
AC = \frac{
\sum{ \left( (F-C) - \overline{(F-C)} \right) \left( (O-C) - \overline{(O-C)} \right)}}
{\sqrt{\sum{ (F-C)^2}} \sqrt{\sum{(O-C)^2}}}  
\end{equation}

Addresses the question: How well did the forecast anomalies correspond to the observed anomalies?

Range: -1 to 1.  Perfect score: 1.

Characteristics: Measures correspondence or phase difference between forecast and observations, \hl{subtracting out the climatological mean at each point, C, rather than the sample mean values}. The anomaly correlation is frequently used to verify output from numerical weather prediction (NWP) models. \hl{AC is not sensitive to forecast bias}, so a good anomaly correlation does not guarantee accurate forecasts. Both forms of the equation are in common use -- see Jolliffe and Stephenson (2012) or Wilks (2011) for further discussion.

In the example above, if the climatological temperature is 14 C, then AC = 0.904. AC is more often used in spatial verification.


https://confluence.ecmwf.int/display/FUG/Anomaly+Correlation+Coefficient

\subsection{Root Mean Square Error}

\begin{equation}
RMSE = \sqrt{ \frac{1}{N} \sum_{i=1}^{N} (F_i - O_i)^2 } = \sqrt{ \overline{ (F-O)^2 } } = \sqrt{MSE}
\end{equation}
Answers the question: What is the average magnitude of the forecast errors?

Range: 0 to âˆž.  Perfect score: 0.

Characteristics: Simple, familiar. Measures "average" error, weighted according to the square of the error. Does not indicate the direction of the deviations. The RMSE puts greater influence on large errors than smaller errors, which may be a good things if large errors are especially undesirable, but may also encourage conservative forecasting.


$MSE = RMSE^2$ can be decomposed into component error sources following \hl{Murphy (1987)}. 

\subsubsection{Decomposition of $MSE$}

\cite{Murphy1988}

\begin{equation}
    MSE = (\overline{F} - \overline{O})^2 + s^2_f + s^2_o - 2s_{fo}
\end{equation}

where $s^2_f = (1/N) \sum (F - \overline{F})^2$ is the sample variance of the forecasts, $s^2_o = (1/N) \sum (O - \overline{O})^2$ is the sample variance of the observations,  $s_{fo} = (1/N) \sum (F - \overline{F}) (O - \overline{O})$ is the sample covariance of the forecasts and observations. Since $s_{fo} = s_f s_o r_{fo}$ where $r_{fo}$ is the sample (Pearson product moment) coefficient of correlation between forecasts and observations, $MSE$ can be further decomposed into:

\begin{equation}
    MSE = (\overline{F} - \overline{O})^2 + s^2_f + s^2_o - 2s_f s_o r_{fo}
\end{equation}



\subsection{Scatter Index}

From \url{http://www.incois.gov.in/HYCOM/1BY12_grid/node5.html}:

Scatter index is calculated by dividing RMSD with mean of the observations at each grid point and multiplying it with 100. It presents the percentage of RMS difference with respect to mean observation or it gives the percentage of expected error for the parameter.

\begin{equation}
    SI = \frac{RMSE}{\overline{O}} &
    SI = \sqrt{ \overline{ (F-O)^2 } }
\end{equation}


\subsection{Skill Score}

From \cite{Murphy1988}

\begin{equation}
    SS = \frac{A_f - A_r}{A_p - A_r}
\end{equation}

Where $A_f$ the accuracy measurement of the forecast being evaluated, $A_r$ is the accuracy of the reference forecast to which is being compared and being $A_p$ the accuracy of a perfect forecast. The skill score based on $MSE$ is also known as \hl{reduction of variance}:

\begin{equation}
    SS = 1 - \frac{MSE_f}{MSE_r}
\end{equation}


Answers the question: What is the relative improvement of the forecast over some reference forecast?

Range: Lower bound depends on what score is being used to compute skill and what reference forecast is used, but upper bound is always 1; 0 indicates no improvement over the reference forecast. Perfect score: 1.

Characteristics: Implies information about the value or worth of a forecast relative to an alternative (reference) forecast. In meteorology the reference forecast is usually persistence (no change from most recent observation) or climatology. The skill score can be unstable for small sample sizes. When MSE is the score used in the above expression then the resulting statistic is called the reduction of variance.

\subsection{Nash-Sutcliffe Efficiency Factor} 

From \url{http://www.cawcr.gov.au/projects/verification/}:

\begin{equation}
E = 1 - \frac{ \sum_{t=1}^{T} (F_t - O_t)^2 }{ \sum_{t=1}^{T} (O_t - \overline{O})^2} = 1 - \frac{MSE}{s^2_O}
\end{equation}

where $MSE = \overline{(F-O)^2}$ and $s^2_O = \overline{ (O-\overline{O})^2 }}$ is the sample variation of observations.

Commonly used in hydrology, with time series. It's the same as $SS$ using $MSE$ as accuracy measurement and the mean of observations as the reference forecast. 

(Nash and Sutcliffe, 1970) 
Answers the question: How well does the forecast predict the observed time series?

Range: -âˆž to 1.  Perfect score: 1.

Characteristics: Frequently used to quantify the accuracy of hydrological predictions. If E=0 then the model forecast is no more accurate than the mean of the observations; if E<0 then the mean observed value is a more accurate predictor than the model. The expression is identical to that for the coefficient of determination R2 and the reduction of variance.

\subsection{Decomposition of $MSE$}

\cite{Murphy1988}

\begin{equation}
    MSE = (\overline{F} - \overline{O})^2 + s^2_f + s^2_o - 2s_{fo}
\end{equation}

where $s^2_f = (1/N) \sum (F - \overline{F})^2$ is the sample variance of the forecasts, $s^2_o = (1/N) \sum (O - \overline{O})^2$ is the sample variance of the observations,  $s_{fo} = (1/N) \sum (F - \overline{F}) (O - \overline{O})$ is the sample covariance of the forecasts and observations. 

With $s_{fo} = s_f s_o r_{fo}$ where $r_{fo}$ is the sample (Pearson product moment) coefficient of correlation between forecasts and observations, $MSE$ can be further decomposed into:

\begin{equation}
    MSE = (\overline{F} - \overline{O})^2 + s^2_f + s^2_o - 2s_f s_o r_{fo}
\end{equation}

\hl{to do}
\begin{equation}
    MSE = BIAS^2 + ....
\end{equation}



%------------------------
%------------------------
%------------------------
\section{Hindcast and Dynamical Downscaling }

\subsection{\cite{VonStorch2017} JGRA - PRINT!!!}

"Global reanalyses suffer from inhomogeneities. However, their large-scale components are mostly homogeneous. Therefore, the concept of downscaling may be applied to homogeneously complement the large-scale state of the reanalyses with regional detail-wherever the condition of homogeneity of the description of large scales is fulfilled. Technically, this can be done by dynamical downscaling using a regional or global climate model, which's large scales are constrained by \hl{spectral nudging}

"When planning for maintenance of off‐shore structures, when designing ships to withstand environmental stresses, when assessing the risk of very high storm surges (Weisse et al. 2009, 2014, 2015) or the potential of wind energy (Geyer et al. 2015; Li et al. 2016a), of the variability of regional upwelling (Tim et al., 2015), implications on permafrost thermal state, ecology and biochemical cycles through snow cover and depth (Klehmet et al. 2013), realistic descriptions of the regionally detailed weather stream for a sufficiently long time are needed. Such descriptions or analyses cannot be constructed directly from observations simply because of scarcity of observational data. 'Long' time series are needed to describe the range of variations, inter-decadal variability and change. In most cases several decades of data will be needed for such assessments. Another condition for such analyses is that they are homogeneous, i.e., changes across time and space are related to variability in weather patterns and frequencies, and not due to changing instrumentation, time of recording, built and natural environment etc. (e.g., Karl et al. 1993). Using a 'frozen' reanalysis model for guiding the analysis is insufficient for guaranteeing homogeneity when the data base is changing in time and space. The latter problem is likely unavoidable."

\subsection{\cite{Zagar2006}}

\textit{Title: Validation of mesoscale low-level winds obtained by dynamical downscaling of ERA40over complex terrain}
 
"Downscaling introduces new scales, both spatial and temporal."

"Dynamical downscaling is a tool that allows for a physically consistent adjustment to new higher resolution terrain, attempting to resolve sub-diurnal variations caused by local features."

There is enhanced spatial variability due to complex terrain and not just synoptic forcing, and temporal variability enhanced through thermal circulations (valley and slope, sea breezes) 


\subsection{\cite{Gomez-Navarro2015}}

\textit{Title:  Sensitivity of the WRF model to PBL parametrisations and nesting techniques: Evaluation of windstorms over complex terrain}

"Surface Wind in complex terrain is difficult to realistically model at coarse resolution and at the same time it's hard to extrapolate local observations onto regular grids."

"Dynamical downscaling is a method that uses RCMs driven at the boundaries and initial state by GCMs but with better representation of local features such as terrain, coastline, land use, soil moisture"

It's currently one of the most cost-effective methods to improve resolution of a coarse global dataset because it's run on a limited area and takes advantage of the observations already assimilated in the GCMs

They cite examples of hindcasts focusing on wind: 

\begin{itemize}
    \item \cite{Jimenez2010}, 
    \item \cite{Garcia-Diez2015}
    \item \cite{Menendez2014}
    \item \cite{Lorente-Plazas2015}
    \item ...
\end{itemize}

They tested 8 different sensitivity studies for several storm events in Switzerland, varying  \gls{PBL} parametrizations, nudging techniques and horizontal resolutions.

They found WRF systematically overestimates wind speed compared to observations, particularly on coarser domains and for maximum wind speed. This problem is greatly reduced in areas of complex terrain by using the \hl{YSU* scheme which accounts for unresolved topography} - \cite{Jimenez2013}. 

They have metrics for wind direction.

We should site as it's a GMD article.

\subsection{\cite{Castro2005}}

\textit{Title: Dynamical downscaling:Assessment of value retained and added using the Regional Atmopsheric Model-ing System (RAMS)}

"the utility of the RCM, or value added, is to resolve the smaller-scale features which have a greater dependence on the surface boundary", i.e., "the surface boundary forcing is the dominant factor in generating atmospheric variability for small-scale features and that it exerts greater control on the RCM solution as the influence of lateral boundary conditions diminish."

In RCMs the added-value comes from the smaller-scale features introduced by the surface boundary, which is particularly dominant in generating variability as the influence of the lateral conditions diminish. 

Hindcast simulations are a particular case of dynamical downscaling where the RCM is driven by coarser reanalysis products, taking advantage of their reliability and assimilated data, but improving simulation of localized features.

\subsection{\cite{Garcia-Diez2015} PRINTED*}

\textit{Title: Assessing and Improving the Local Added Value of WRF for Wind Downscaling}

Compares WRF 8~km from FGS, ERA-Interim and NCEP-NCA. 

"Added value of wind downscaling depends on the geographical complexity of the area under study" 

"Results suggest that, as model resolution increases, traditional skill scores tend to saturate. Thus, adding value to high-resolution global models becomes significantly more difficult." 

It's easier to add value to coarse resolutions in complex terrain than in flat, or in coast line than offshore for a marine dataset. 

They provide reasons such as no data assimilation to reduce spinup period and warm start of soil moisture (i.e. not "cold" start from coarse data every run).

\subsubsection{Verification}

Explored verification with spectral analysis in 3 bands: synoptic, diurnal and semi-diurnal. But they didn't used this in the paper (Garcia..) because only had 6-hour reanalysis data.

Also, they compared with station data (and buoys) because wind is a variable with local features and interpolation is a challenge for areal-representative gridded observations (needs a dense network of stations to be realistic. They used nearest-neighbour interpolation filtering land/ocean mask for coastal stations that \hl{significantly improved coastal stations results}. Overall bias is positive but there are 2 stations with negative bias and they say it's because they are \hl{"windy stations located on capes and surrounded by cliffs, obstacles that are not resolved by the model."}

They also compared different time aggregations to account for the different frequencies of available data: instantaneous 6 hours, averaged 6 hours and daily.

They ran 36~h daily (12z) simulations, with 12~h spinup, and provide references for studies that did the same and provide good results even comparing with hindcasts that used \hl{nudging} such as \cite{Menendez2014}. "This scheme is computationally cheap but sacrifices small-scale realist from slow-varying variables such as soil moisture".  

It also says that 6-12h is enough spinup for atmospheric variables.

\subsection{\cite{Vincent2015}}

"Simulations of the regional wind climate, like all regional climate simulations, can be constrained to the large-scale flow by regular and frequent initialization of the model from large-scale forcing, with the first part of each model run being discarded as a 'spinup’ period, during which time the scales resolved in the simulation transition from only those in the large-scale forcing to the full effective resolution of the smaller-scale model. The advantage of this method is that simulations are short enough to prevent the interior of the mesoscale model diverging from the large-scale circulation patterns, but the disadvantages are wasted computational power for the spinup period and discontinuities between individual simulations. Alternatively, the mesoscale simulations can be run continuously without reinitialization, but as shown by Lo et al. (2008) and Bowden et al. (2012, 2013), this can result in drift from the large-scale circulation patterns. It has been shown that these problems can be alleviated by nudging the regional climate model toward its difference from the large-scale forcing, either by nudging in gridpoint space or by nudging in spectral space so that only wavelengths above a certain threshold are nudged."

"Despite the advantages of nudging in the Weather Research and Forecasting (WRF) Model, there is a risk that some of the variability in the regional climate model will be damped by the nudging. For example, Bowden et al. (2012) suggested that nudging could reduce errors at the expense of reducing variability, although..."

"The results suggest that running the model for 10-day periods without reinitialization and with grid or spectral nudging applied to an outer nest is a reasonable configuration for nested regional climate simulations that is comparable to short runs with daily reinitialization discarding the first 12h, although care should be taken near the edges of the domain."

"The long-reinitialization method saves considerable computer resources and results in time series that are more consistent with each other."

"grid nudging causes smoothing of the wind in the 15-km domain at all wavenumbers, both at 1150m AGL and near the surface where nudging is not applied directly, while spectral nudging mainly affects longer wavenumbers. Maps of mesoscale variance show spatial smoothing for both grid and spectral nudging, although the effect is less pronounced for spectral nudging. On the inner, 5-km domain, an indirect smoothing impact of nudging is seen up to 200km inward from the dominant inflow boundary at 1150mAGL,but there is minimal smoothing from the nudging near the surface, indicating that nudging an outer domain is an appropriate configuration for wind-resource modeling."

Comparing daily initialisation with grid and \hl{spectral nudging} with WRF over the Baltic Sea, concluding that grid-nudging without reinitialisation is comparable to short runs with daily reinitialisation discarding the first 12h, for regions sufficiently far away from the boundaries. However, grid-nudging spatially smooths the wind filed on the outer nest and can damp variability and parallelising the frequent initialisation runs optimises the use of computational resources, making it a more appealing approach for small computational centres.

\subsection{\cite{El-Samra2018}}

Applied WRF to Eastern Mediterraean to see what is the best resolution to simulate complex terrain, with 9,3 and 1 km nesting

Great improvement at 1 and 3 km over the 9km particularly for precipitation and temperature. 

"Wind speeds, on the other hand, are generally overestimated for all model resolutions, in comparison with observational data, particularly on the coast (up to 50\%) compared to inland stations (up to 40\%). The findings therefore indicate that a 3km resolution is sufficient for the downscaling, especially that it would allow more years and scenarios to be investigated compared to the higher 1km resolution at the same computational effort."

El-Samara showed WRF generally overestimates wind speed in complex terrain, more so near the coast than inland. They also show that improvements on wind speed plateau for resolutions under 3 km.



% --------------- Section --------------- %
\section{Added value of RCMs}

\subsection{\cite{Winterfeldt2009} MWR}

\textit{Title: Assessment of Value Added for Surface Marine Wind Speed Obtained from Two Regional Climate Models}

Found added value of RCM near coast with verification with buoy data.

\subsection{\cite{Winterfeldt2011} IJC }

\textit{Title: Using QuikSCAT in the added value assessment of dynamically downscaled wind speed}

Found added value of RCM near coast with satellite measurements and RCM with 1.875 degrees resolution. 

\subsection{\cite{Feser2011} BAMS}

\textit{Title: Regional Climate ModelsAdd Value to Global Model Data: A Review and Selected Examples}

"Regional models can add value, but only for certain variables and locations—particularly those influenced by regional specifics, such as coasts, or mesoscale dynamics, such as polar lows. Therefore, the decision of whether a regional climate model simulation is required depends crucially on the scientific question being addressed."

\subsection{\cite{Horvath2011} JAMC}

\textit{Title: Dynamical downscaling of wind speed in complex terrain prone to bora-type flows}

Downscaled ERA-40 with ALADIN to 8~km and DADA (dynamical adaptation) to 2\~km. 

They found that BIAS for DADA is $1~\%$ of mean wind speed in flat terrain, reaching up to $10~\%$ for complex terrain, mainly because of underestimation of strongest winds. RMSE for DADA is about $12~\%$ for complex terrain. They also made a \hl{spectral decomposition of wind} and verified that cross-mountain winds were better simulated with dynamical adaptation and mixed results for along-mountain winds. Also, the mesoscale model improved energy on smaller scales of motion, namely secondary diurnal and tertiary semidiurnal maxima particularly on the coast, not on the continent, and underestimated less-than-semidiurnal periods.


% --------------- Section --------------- %
\section{Marine Hindcasts}

\subsection{\cite{Lewis2017}}

% Title: Impactos associados À resoluÇÃo dos modelos atmosfÉricos emmodelos de prognÓsticos de ondas

"Na modelagem atmosférica alguns autores compararam os resultados de simulações com diferentes resoluções espaciais e dados observados, dentre eles destacam-se \hl{Zhong et al. (2003), Mass et al. (2002) e Rife et al. (2005)}. Todos os autores concluíram que o aumento da resolução espacial apresentou resultados com erros menores. Mass et al. (2002) completa ainda que o ganho ocorreu ao diminuir o espaçamento de grade de 32 km para 12 km, mas que ao aumentar resolução de 12 km para 4 km não observou vantagens, sugerindo que a resolução ideal do modelo atmosférico para a região analisada fosse entre 10 e 15 km."


\subsection{\cite{Menendez2014} CD PRINTED**}

\textit{Title: High-resolution sea wind hindcasts over the Mediterranean area}

Did a long-term study and realized that WRF only had value in a few points near the coastline?

Sensitivity test from Oct-Dec 2001 (super storm)

daily forecast outperforms spectral technique of previous products (??)

ERA interim has better estimates in terms of wind variability and hour-to-hour correspondence. 

Recent studies have been comparing the performance of hindcast driven by \gls{CFSR}, ERA-40 and ERA-Intermin, with the conclusion that ERA-Intermim shows more skill and \gls{CFSR} performs worse but not significantly enough to be discarded. In fact, it could be prefered as it provides the longest period. 

"A daily reforecast running mode represented wind variability and hour-to-hour correspondence better than simulations running freely in "climate" mode, using spectral nudging, and to a lesser extent, also better than the run using grid nudging"

"A Validation of the hindcast data was carried out in order to evaluate their reliability. The wind hindcasts were validated by comparing them with both, in situ hourly stations and wind altimeter observations from satellites."

% --------------- Section --------------- %
\section{Verification:}

\subsection{\cite{Mass2002} BAMS}

\textit{Title: Does increasinghorizontal resolution produce more skillful forecasts?}

Traditional skill scores such as RMSE are not the best to assess added value because they are very sensitive to phase errors and as high-resolution produce sharper changes, scores such as RMSE appear to be worse than coarse models

\subsection{\cite{Rife2005} MWR}

\textit{Title: Verification of Temporal Variations in Mesoscale Numerical Wind Forecasts.}

Show different verification methods (\hl{anomaly correlation})

\subsection{other referenced before:}

\cite{Garcia-Diez2015} 

\cite{Winterfeldt2009}

\cite{Horvath2011} used spectral decomposition of wind


%Print the glossary
\printglossaries
%------------- BIBLIO -------------%
% \clearpage
\bibliographystyle{chicago}
\bibliography{references.bib}
\label{sec:references}
% %% command                        & example result
% %% \citet{jones90}|               & Jones et al. (1990)
% %% \citep{jones90}|               & (Jones et al., 1990)
% %% \citep{jones90,jones93}|       & (Jones et al., 1990, 1993)
% %% \citep[p.~32]{jones90}|        & (Jones et al., 1990, p.~32)
% %% \citep[e.g.,][]{jones90}|      & (e.g., Jones et al., 1990)
% %% \citep[e.g.,][p.~32]{jones90}| & (e.g., Jones et al., 1990, p.~32)
% %% \citeauthor{jones90}|          & Jones et al.
% %% \citeyear{jones90}|            & 1990

\end{document}
